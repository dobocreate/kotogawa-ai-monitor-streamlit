"""
AIäºˆæ¸¬ç²¾åº¦è©•ä¾¡ãƒšãƒ¼ã‚¸
å„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡ãƒ»æ¯”è¼ƒã—ã¾ã™
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from pathlib import Path
import sys
import json

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from scripts.prediction_evaluator import PredictionEvaluator
    from scripts.advanced_prediction import AdvancedRiverLevelPredictor
    from scripts.river_streaming_prediction import RiverStreamingPredictor
    from scripts.collect_data import DataCollector
    MODULES_AVAILABLE = True
except ImportError:
    MODULES_AVAILABLE = False
    
# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="äºˆæ¸¬ç²¾åº¦è©•ä¾¡ - åšæ±å·ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ“ˆ",
    layout="wide"
)

def load_evaluation_data():
    """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    evaluator = PredictionEvaluator()
    
    # ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
    eval_file = "data/evaluation_results.json"
    if Path(eval_file).exists():
        evaluator.load_from_file(eval_file)
        
    return evaluator

def run_backtest(evaluator: PredictionEvaluator, history_data: list, 
                start_time: datetime, end_time: datetime):
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    
    # é€²æ—è¡¨ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–
    expert_predictor = AdvancedRiverLevelPredictor()
    river_predictor = RiverStreamingPredictor()
    
    # ãƒ†ã‚¹ãƒˆæœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    test_data = []
    for data in history_data:
        data_time = datetime.fromisoformat(data.get('data_time', data.get('timestamp')))
        if start_time <= data_time <= end_time:
            test_data.append(data)
            
    if len(test_data) < 50:
        st.error("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½50ä»¶å¿…è¦ï¼‰")
        return
        
    # 10åˆ†é–“éš”ã§ãƒ†ã‚¹ãƒˆ
    total_tests = len(test_data) - 30  # äºˆæ¸¬ã«å¿…è¦ãªå±¥æ­´ã‚’ç¢ºä¿
    completed_tests = 0
    
    for i in range(30, len(test_data) - 18):  # 3æ™‚é–“å…ˆã¾ã§ã®äºˆæ¸¬ãŒå¯èƒ½ãªç¯„å›²
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿
        history = test_data[:i]
        
        # å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆ3æ™‚é–“å…ˆã¾ã§ï¼‰
        actual = test_data[i:i+18]
        
        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«äºˆæ¸¬
        status_text.text(f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«äºˆæ¸¬ã‚’è©•ä¾¡ä¸­... ({i-29}/{total_tests})")
        expert_predictions = expert_predictor.predict(history)
        if expert_predictions:
            evaluator.evaluate_prediction(expert_predictions, actual, 'expert_rule')
            
        # Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’äºˆæ¸¬
        status_text.text(f"Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’äºˆæ¸¬ã‚’è©•ä¾¡ä¸­... ({i-29}/{total_tests})")
        # å­¦ç¿’ã‚’å®Ÿè¡Œ
        if i > 50:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿å­¦ç¿’
            river_predictor.learn(history[:-20])  # æœ€æ–°20ä»¶ã‚’é™¤ã„ã¦å­¦ç¿’
            
        river_predictions = river_predictor.predict(history)
        if river_predictions:
            evaluator.evaluate_prediction(river_predictions, actual, 'river_online')
            
        # é€²æ—æ›´æ–°
        completed_tests += 1
        progress_bar.progress(completed_tests / total_tests)
        
    # è©•ä¾¡çµæœã‚’ä¿å­˜
    evaluator.save_to_file("data/evaluation_results.json")
    
    progress_bar.empty()
    status_text.text("è©•ä¾¡å®Œäº†ï¼")
    
    return evaluator

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    st.title("ğŸ“ˆ AIäºˆæ¸¬ç²¾åº¦è©•ä¾¡")
    st.markdown("å„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’è©•ä¾¡ãƒ»æ¯”è¼ƒã—ã¾ã™ã€‚")
    
    if not MODULES_AVAILABLE:
        st.error("å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã€‚")
        return
        
    # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    evaluator = load_evaluation_data()
    
    # ã‚¿ãƒ–æ§‹æˆ
    tab1, tab2, tab3, tab4 = st.tabs(["ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è©•ä¾¡", "ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ", "è©³ç´°æ¯”è¼ƒ", "æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰"])
    
    with tab1:
        show_realtime_evaluation(evaluator)
        
    with tab2:
        show_backtest(evaluator)
        
    with tab3:
        show_detailed_comparison(evaluator)
        
    with tab4:
        show_performance_trend(evaluator)
        
def show_realtime_evaluation(evaluator: PredictionEvaluator):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è©•ä¾¡ã®è¡¨ç¤º"""
    st.header("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è©•ä¾¡")
    
    # æœ€æ–°ã®è©•ä¾¡çµæœã‚’å–å¾—
    summary = evaluator.get_comparison_summary()
    
    if not summary or not any(data.get('latest') for data in summary.values()):
        st.info("ã¾ã è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’ç¨¼åƒã•ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã—ã¦ãã ã•ã„ã€‚")
        return
        
    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚«ãƒ¼ãƒ‰
    col1, col2 = st.columns(2)
    
    with col1:
        show_model_card("ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«äºˆæ¸¬", summary.get('expert_rule', {}), "blue")
        
    with col2:
        show_model_card("Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’äºˆæ¸¬", summary.get('river_online', {}), "green")
        
    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    st.subheader("ğŸ“Š è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
    
    # æ¯”è¼ƒè¡¨
    comparison_df = evaluator.get_detailed_comparison()
    if not comparison_df.empty:
        # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã§è¦‹ã‚„ã™ãè¡¨ç¤º
        pivot_df = comparison_df.pivot(index='äºˆæ¸¬æ™‚é–“', columns='ãƒ¢ãƒ‡ãƒ«', values=['MAE (m)', 'RMSE (m)', 'MAPE (%)'])
        st.dataframe(pivot_df, use_container_width=True)
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        fig = px.line(comparison_df, x='äºˆæ¸¬æ™‚é–“', y='MAE (m)', 
                     color='ãƒ¢ãƒ‡ãƒ«', markers=True,
                     title="äºˆæ¸¬æ™‚é–“åˆ¥ã®å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰")
        st.plotly_chart(fig, use_container_width=True)
        
def show_model_card(model_name: str, model_data: dict, color: str):
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚«ãƒ¼ãƒ‰ã®è¡¨ç¤º"""
    latest = model_data.get('latest', {})
    average = model_data.get('average', {})
    
    # ã‚«ãƒ¼ãƒ‰ã®ã‚¹ã‚¿ã‚¤ãƒ«
    card_style = f"""
    <div style="
        background-color: {'#e3f2fd' if color == 'blue' else '#e8f5e9'};
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid {color};
    ">
    """
    
    st.markdown(card_style, unsafe_allow_html=True)
    st.subheader(model_name)
    
    if latest and 'overall' in latest:
        overall = latest['overall']
        
        # ã‚¹ã‚³ã‚¢è¡¨ç¤ºï¼ˆå¤§ããï¼‰
        score = overall.get('score', 0)
        score_color = "green" if score >= 80 else "orange" if score >= 60 else "red"
        st.markdown(f"### ç²¾åº¦ã‚¹ã‚³ã‚¢: <span style='color: {score_color}; font-size: 36px;'>{score:.1f}</span> / 100", 
                   unsafe_allow_html=True)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("MAE", f"{overall['mae']:.3f} m", 
                     help="å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰")
            
        with col2:
            st.metric("RMSE", f"{overall['rmse']:.3f} m",
                     help="äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰")
            
        with col3:
            st.metric("MAPE", f"{overall['mape']:.1f} %",
                     help="å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰")
            
        # å¹³å‡å€¤ã¨ã®æ¯”è¼ƒ
        if average:
            st.markdown("**æœ€è¿‘10å›ã®å¹³å‡:**")
            st.text(f"MAE: {average['mae']:.3f} m, RMSE: {average['rmse']:.3f} m, ã‚¹ã‚³ã‚¢: {average['score']:.1f}")
            
        # è©•ä¾¡æƒ…å ±
        eval_time = datetime.fromisoformat(latest['evaluated_at'])
        st.caption(f"æœ€çµ‚è©•ä¾¡: {eval_time.strftime('%Y-%m-%d %H:%M:%S')}")
        st.caption(f"è©•ä¾¡å›æ•°: {model_data.get('evaluation_count', 0)}å›")
        
    else:
        st.info("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        
    st.markdown("</div>", unsafe_allow_html=True)
    
def show_backtest(evaluator: PredictionEvaluator):
    """ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã¨è¡¨ç¤º"""
    st.header("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ")
    st.markdown("""
    éå»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
    ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆ2023å¹´6æœˆ25æ—¥ã€œ7æœˆ2æ—¥ï¼‰ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    """)
    
    # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨­å®š
    col1, col2, col3 = st.columns(3)
    
    with col1:
        test_start = st.date_input(
            "é–‹å§‹æ—¥",
            value=datetime(2023, 6, 26).date(),
            min_value=datetime(2023, 6, 25).date(),
            max_value=datetime(2023, 7, 1).date()
        )
        
    with col2:
        test_end = st.date_input(
            "çµ‚äº†æ—¥",
            value=datetime(2023, 6, 28).date(),
            min_value=datetime(2023, 6, 25).date(),
            max_value=datetime(2023, 7, 1).date()
        )
        
    with col3:
        st.write("")  # ã‚¹ãƒšãƒ¼ã‚µãƒ¼
        if st.button("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ", type="primary"):
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            with st.spinner("éå»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                history_data = load_demo_data()
                
            if history_data:
                # ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                start_dt = datetime.combine(test_start, datetime.min.time())
                end_dt = datetime.combine(test_end, datetime.max.time())
                
                updated_evaluator = run_backtest(evaluator, history_data, start_dt, end_dt)
                
                # çµæœã‚’è¡¨ç¤º
                st.success("ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
                st.rerun()
            else:
                st.error("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
                
def load_demo_data():
    """ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    collector = DataCollector()
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
    water_df = pd.read_csv('sample/water-level_20230625-20230701.csv')
    dam_df = pd.read_csv('sample/dam_20230625-20230701.csv')
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å¤‰æ›
    history_data = []
    
    for idx, row in water_df.iterrows():
        timestamp = pd.to_datetime(row['æ—¥æ™‚']).isoformat()
        
        # å¯¾å¿œã™ã‚‹ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        dam_row = dam_df[dam_df['æ—¥æ™‚'] == row['æ—¥æ™‚']]
        
        data = {
            'data_time': timestamp,
            'river': {
                'water_level': float(row['æ°´ä½(m)']) if pd.notna(row['æ°´ä½(m)']) else None
            }
        }
        
        if not dam_row.empty:
            data['dam'] = {
                'outflow': float(dam_row.iloc[0]['å…¨æ”¾æµé‡(m3/s)']) if pd.notna(dam_row.iloc[0]['å…¨æ”¾æµé‡(m3/s)']) else None
            }
            
        history_data.append(data)
        
    return history_data
    
def show_detailed_comparison(evaluator: PredictionEvaluator):
    """è©³ç´°æ¯”è¼ƒã®è¡¨ç¤º"""
    st.header("è©³ç´°æ¯”è¼ƒ")
    
    summary = evaluator.get_comparison_summary()
    
    if not summary:
        st.info("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
        
    # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ¯”è¼ƒ
    st.subheader("äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—åˆ¥ç²¾åº¦")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    step_data = []
    for model_type, model_name in [('expert_rule', 'ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«'), 
                                   ('river_online', 'Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’')]:
        if model_type in summary and summary[model_type].get('latest'):
            latest = summary[model_type]['latest']
            if 'by_step' in latest:
                for step, metrics in latest['by_step'].items():
                    minutes = int(step.replace('min', ''))
                    step_data.append({
                        'ãƒ¢ãƒ‡ãƒ«': model_name,
                        'äºˆæ¸¬æ™‚é–“ï¼ˆåˆ†ï¼‰': minutes,
                        'MAE': metrics['mae'],
                        'RMSE': metrics['rmse'],
                        'MAPE': metrics['mape']
                    })
                    
    if step_data:
        step_df = pd.DataFrame(step_data)
        
        # MAEã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig1 = px.line(step_df, x='äºˆæ¸¬æ™‚é–“ï¼ˆåˆ†ï¼‰', y='MAE', color='ãƒ¢ãƒ‡ãƒ«',
                      title='äºˆæ¸¬æ™‚é–“åˆ¥ å¹³å‡çµ¶å¯¾èª¤å·®ï¼ˆMAEï¼‰',
                      markers=True)
        fig1.update_yaxis(title='MAE (m)')
        st.plotly_chart(fig1, use_container_width=True)
        
        # RMSEã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig2 = px.line(step_df, x='äºˆæ¸¬æ™‚é–“ï¼ˆåˆ†ï¼‰', y='RMSE', color='ãƒ¢ãƒ‡ãƒ«',
                      title='äºˆæ¸¬æ™‚é–“åˆ¥ äºŒä¹—å¹³å‡å¹³æ–¹æ ¹èª¤å·®ï¼ˆRMSEï¼‰',
                      markers=True)
        fig2.update_yaxis(title='RMSE (m)')
        st.plotly_chart(fig2, use_container_width=True)
        
        # MAPEã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
        fig3 = px.line(step_df, x='äºˆæ¸¬æ™‚é–“ï¼ˆåˆ†ï¼‰', y='MAPE', color='ãƒ¢ãƒ‡ãƒ«',
                      title='äºˆæ¸¬æ™‚é–“åˆ¥ å¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®ï¼ˆMAPEï¼‰',
                      markers=True)
        fig3.update_yaxis(title='MAPE (%)')
        st.plotly_chart(fig3, use_container_width=True)
        
    # çµ±è¨ˆçš„åˆ†æ
    st.subheader("çµ±è¨ˆçš„åˆ†æ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if 'expert_rule' in summary and summary['expert_rule'].get('latest'):
            latest = summary['expert_rule']['latest']['overall']
            st.markdown("**ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«äºˆæ¸¬**")
            st.write(f"- ãƒã‚¤ã‚¢ã‚¹: {latest.get('bias', 0):.3f} m")
            st.write(f"- æ¨™æº–åå·®: {latest.get('std', 0):.3f} m")
            st.write(f"- ã‚µãƒ³ãƒ—ãƒ«æ•°: {latest.get('count', 0)}")
            
    with col2:
        if 'river_online' in summary and summary['river_online'].get('latest'):
            latest = summary['river_online']['latest']['overall']
            st.markdown("**Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’äºˆæ¸¬**")
            st.write(f"- ãƒã‚¤ã‚¢ã‚¹: {latest.get('bias', 0):.3f} m")
            st.write(f"- æ¨™æº–åå·®: {latest.get('std', 0):.3f} m")
            st.write(f"- ã‚µãƒ³ãƒ—ãƒ«æ•°: {latest.get('count', 0)}")
            
def show_performance_trend(evaluator: PredictionEvaluator):
    """æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¡¨ç¤º"""
    st.header("æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é¸æŠ
    metric = st.selectbox(
        "è©•ä¾¡æŒ‡æ¨™",
        ["mae", "rmse", "score"],
        format_func=lambda x: {"mae": "MAE", "rmse": "RMSE", "score": "ç²¾åº¦ã‚¹ã‚³ã‚¢"}[x]
    )
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    trend_data = []
    for model_type, model_name in [('expert_rule', 'ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«'), 
                                   ('river_online', 'Riverã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’')]:
        df = evaluator.get_performance_trend(model_type, metric)
        if not df.empty:
            df['ãƒ¢ãƒ‡ãƒ«'] = model_name
            trend_data.append(df)
            
    if trend_data:
        # ãƒ‡ãƒ¼ã‚¿çµåˆ
        trend_df = pd.concat(trend_data, ignore_index=True)
        
        # ã‚°ãƒ©ãƒ•ä½œæˆ
        fig = px.line(trend_df, x='datetime', y=metric, color='ãƒ¢ãƒ‡ãƒ«',
                     title=f'{metric.upper()}ã®æ¨ç§»',
                     markers=True)
        
        # Yè»¸ã®ãƒ©ãƒ™ãƒ«
        if metric == 'mae':
            fig.update_yaxis(title='MAE (m)')
        elif metric == 'rmse':
            fig.update_yaxis(title='RMSE (m)')
        else:
            fig.update_yaxis(title='ç²¾åº¦ã‚¹ã‚³ã‚¢')
            
        fig.update_xaxis(title='è©•ä¾¡æ—¥æ™‚')
        st.plotly_chart(fig, use_container_width=True)
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        st.subheader("çµ±è¨ˆã‚µãƒãƒªãƒ¼")
        summary_df = trend_df.groupby('ãƒ¢ãƒ‡ãƒ«')[metric].agg(['mean', 'std', 'min', 'max']).round(3)
        summary_df.columns = ['å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°', 'æœ€å¤§']
        st.dataframe(summary_df, use_container_width=True)
        
    else:
        st.info("ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç¶™ç¶šçš„ã«è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()